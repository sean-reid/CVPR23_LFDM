{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3.8",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install all the needed dependencies"
      ],
      "metadata": {
        "id": "1YGOANKbZEUk"
      }
    },
    {
        "cell_type": "code",
        "source": [
            "!sudo apt-get update -y\n",
            "!sudo apt-get install python3.8\n\n",
            "from IPython.display import clear_output\n",
            "clear_output()\n\n",
            "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n\n",
            "# Choose one of the given alternatives:\n",
            "!sudo update-alternatives --config python3\n\n",

            "# This one used to work but now NOT(for me)!\n",
            "# !sudo update-alternatives --config python\n\n",

            "# Check the result\n",
            "!python3 --version\n\n",

            "# Attention: Install pip (... needed!)\n",
            "!sudo apt install python3-pip\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIOWNlG7Y_gZ",
        "outputId": "2cb559c8-d03d-414b-a5b0-ac316c48d517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1dRn1wl5TUaZJiiDpIQADt1JJ0_q36MVG\n",
        "!gdown 1lPVIT_cXXeOVogKLhD9fAT4k1Brd_HHn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6qovP0AaKtv",
        "outputId": "1d63317a-be9f-4631-c28b-e631514e024a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dRn1wl5TUaZJiiDpIQADt1JJ0_q36MVG\n",
            "To: /content/LFAE_MUG.pth\n",
            "100% 792M/792M [00:06<00:00, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lPVIT_cXXeOVogKLhD9fAT4k1Brd_HHn\n",
            "To: /content/DM_MUG.pth\n",
            "100% 513M/513M [00:08<00:00, 59.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dependencies"
      ],
      "metadata": {
        "id": "fAZeosjIZIIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "import os\n",
        "import timeit\n",
        "from PIL import Image\n",
        "from misc import Logger, grid2fig, conf2fig\n",
        "import random\n",
        "from DM.modules.video_flow_diffusion_model import FlowDiffusion\n",
        "from misc import resize\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kc8TI45_ZDhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timeit.default_timer()\n",
        "GPU = \"1\"\n",
        "postfix = \"-j-sl-random-of-tr-rmm\"\n",
        "INPUT_SIZE = 128\n",
        "N_FRAMES = 40\n",
        "RANDOM_SEED = 1234\n",
        "MEAN = (0.0, 0.0, 0.0)\n",
        "cond_scale = 1.\n",
        "# downloaded the pretrained DM model and put its path here\n",
        "RESTORE_FROM = \"/content/DM_MUG.pth\"\n",
        "# downloaded the pretrained LFAE model and put its path here\n",
        "AE_RESTORE_FROM = \"/content/LFAE_MUG.pth\"\n",
        "config_path = \"config/mug128.yaml\"\n",
        "CKPT_DIR = os.path.join(\"checkpoints\", \"demo\"+postfix)\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "print(\"RESTORE_FROM:\", RESTORE_FROM)\n",
        "print(\"AE_RESTORE_FROM:\", AE_RESTORE_FROM)\n",
        "print(\"cond scale:\", cond_scale)"
      ],
      "metadata": {
        "id": "gp7Wa86UZdKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_img(rec_img_batch, index):\n",
        "    rec_img = rec_img_batch[index].permute(1, 2, 0).data.cpu().numpy().copy()\n",
        "    rec_img += np.array(MEAN)/255.0\n",
        "    rec_img[rec_img < 0] = 0\n",
        "    rec_img[rec_img > 1] = 1\n",
        "    rec_img *= 255\n",
        "    return np.array(rec_img, np.uint8)\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "V0--YfK7cQ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Create the model and start the training.\"\"\"\n",
        "\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU\n",
        "\n",
        "    cudnn.enabled = True\n",
        "    cudnn.benchmark = True\n",
        "    setup_seed(RANDOM_SEED)\n",
        "\n",
        "    model = FlowDiffusion(is_train=True,\n",
        "                          sampling_timesteps=1000,\n",
        "                          config_pth=config_path,\n",
        "                          pretrained_pth=AE_RESTORE_FROM)\n",
        "    model.cuda()\n",
        "\n",
        "    if RESTORE_FROM:\n",
        "        if os.path.isfile(RESTORE_FROM):\n",
        "            print(\"=> loading checkpoint '{}'\".format(RESTORE_FROM))\n",
        "            checkpoint = torch.load(RESTORE_FROM)\n",
        "            model.diffusion.load_state_dict(checkpoint['diffusion'])\n",
        "            print(\"=> loaded checkpoint '{}'\".format(RESTORE_FROM))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(RESTORE_FROM))\n",
        "            exit(-1)\n",
        "    else:\n",
        "        print(\"NO checkpoint found!\")\n",
        "        exit(-1)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    exp_list = ['anger', 'disgust', 'fear', 'happiness',\n",
        "                'neutral', 'sadness', 'surprise']\n",
        "\n",
        "    ref_img_path = \"demo/mug_examples/img_0000.jpg\"\n",
        "    print(\"input image:\", ref_img_path)\n",
        "    ref_img_name = os.path.basename(ref_img_path)[:-4]\n",
        "    ref_img_npy = imageio.v2.imread(ref_img_path)[:, :, :3]\n",
        "    ref_img_npy = resize(ref_img_npy, 128, interpolation=cv2.INTER_AREA)\n",
        "    ref_img_npy = np.asarray(ref_img_npy, np.float32)\n",
        "    ref_img_npy = ref_img_npy - np.array(MEAN)\n",
        "    ref_img = torch.from_numpy(ref_img_npy/255.0)\n",
        "    ref_img = ref_img.permute(2, 0, 1).float()\n",
        "    ref_imgs = ref_img.unsqueeze(dim=0).cuda()\n",
        "\n",
        "    nf = 40\n",
        "    cnt = 0\n",
        "    for ref_text in exp_list:\n",
        "        model.set_sample_input(sample_img=ref_imgs, sample_text=[ref_text])\n",
        "        model.sample_one_video(cond_scale=cond_scale)\n",
        "        msk_size = ref_imgs.shape[-1]\n",
        "        save_src_img = sample_img(ref_imgs, 0)\n",
        "        new_im_list = []\n",
        "        for frame_idx in range(nf):\n",
        "            save_sample_out_img = sample_img(model.sample_out_vid[:, :, frame_idx], 0)\n",
        "            save_sample_warp_img = sample_img(model.sample_warped_vid[:, :, frame_idx], 0)\n",
        "            save_fake_grid = grid2fig(model.sample_vid_grid[0, :, frame_idx].permute((1, 2, 0)).data.cpu().numpy(),\n",
        "                                      grid_size=32, img_size=msk_size)\n",
        "            save_fake_conf = conf2fig(model.sample_vid_conf[0, :, frame_idx])\n",
        "            new_im = Image.new('RGB', (msk_size * 5, msk_size))\n",
        "            new_im.paste(Image.fromarray(save_src_img, 'RGB'), (0, 0))\n",
        "            new_im.paste(Image.fromarray(save_sample_out_img, 'RGB'), (msk_size * 1, 0))\n",
        "            new_im.paste(Image.fromarray(save_sample_warp_img, 'RGB'), (msk_size * 2, 0))\n",
        "            new_im.paste(Image.fromarray(save_fake_grid, 'RGB'), (msk_size * 3, 0))\n",
        "            new_im.paste(Image.fromarray(save_fake_conf, 'L'), (msk_size * 4, 0))\n",
        "            new_im_arr = np.array(new_im)\n",
        "            new_im_list.append(new_im_arr)\n",
        "        video_name = \"%04d_%s_%s_%.2f.gif\" % (cnt, ref_text, ref_img_name, cond_scale)\n",
        "        print(video_name)\n",
        "        imageio.mimsave(os.path.join(CKPT_DIR, video_name), new_im_list)\n",
        "        cnt += 1\n"
      ],
      "metadata": {
        "id": "mjC0zj1gcSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "AQfx3SBbdAKm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
